{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Evaluation Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from factor_analyzer.rotator import Rotator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sample = 50\n",
    "n_factors = [5, 4, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-forward Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import done, now continue with estimation and then finish with eval. Then copy what you've done here over to the other notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "directory = f'Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42'\n",
    "files = os.listdir(directory)\n",
    "\n",
    "pattern_corr = re.compile(r'^(df\\d{2}[a-z])_varresid_(is|os)_corr_(is|os)\\.csv$')\n",
    "pattern_resid = re.compile(r'^(df\\d{2}[a-z])_varresid_(is|os)\\.csv$')\n",
    "\n",
    "for fname in files:\n",
    "    # Skip covariance matrices (those with 'cov_' in the filename)\n",
    "    if 'cov_' in fname:\n",
    "        continue\n",
    "    full_path = os.path.join(directory, fname)\n",
    "    \n",
    "    m = pattern_corr.match(fname)\n",
    "    if m:\n",
    "        # File: df00a_varresid_is_corr_is.csv -> Key: corr00a_is \n",
    "        base = m.group(1)    # e.g. \"df00a\"\n",
    "        inout = m.group(2)   # e.g. \"is\" or \"os\"\n",
    "        new_key = f\"corr{base[2:]}_{inout}\"\n",
    "        df = pd.read_csv(full_path, index_col=0)\n",
    "        globals()[new_key] = df\n",
    "        continue\n",
    "\n",
    "    m = pattern_resid.match(fname)\n",
    "    if m:\n",
    "        # File: df00a_varresid_is.csv -> Key: df00a_is\n",
    "        base = m.group(1)\n",
    "        inout = m.group(2)\n",
    "        new_key = f\"{base}_{inout}\"\n",
    "        df = pd.read_csv(full_path, index_col=0)\n",
    "        globals()[new_key] = df\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for rotation here yet because it doesn't change the explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following number of factors per period:\n",
      "Period 00: 5 factors\n",
      "Period 05: 4 factors\n",
      "Period 10: 4 factors\n",
      "Period 15: 5 factors\n",
      "\n",
      "\n",
      "Completed 00: in-sample 'a' vs out-of-sample 'b'\n",
      "Completed 00: in-sample 'b' vs out-of-sample 'c'\n",
      "Completed 00: in-sample 'c' vs out-of-sample 'd'\n",
      "Completed 00: in-sample 'd' vs out-of-sample 'e'\n",
      "Completed 05: in-sample 'a' vs out-of-sample 'b'\n",
      "Completed 05: in-sample 'b' vs out-of-sample 'c'\n",
      "Completed 05: in-sample 'c' vs out-of-sample 'd'\n",
      "Completed 05: in-sample 'd' vs out-of-sample 'e'\n",
      "Completed 10: in-sample 'a' vs out-of-sample 'b'\n",
      "Completed 10: in-sample 'b' vs out-of-sample 'c'\n",
      "Completed 10: in-sample 'c' vs out-of-sample 'd'\n",
      "Completed 10: in-sample 'd' vs out-of-sample 'e'\n",
      "Completed 15: in-sample 'a' vs out-of-sample 'b'\n",
      "Completed 15: in-sample 'b' vs out-of-sample 'c'\n",
      "Completed 15: in-sample 'c' vs out-of-sample 'd'\n",
      "Completed 15: in-sample 'd' vs out-of-sample 'e'\n",
      "\n",
      "Results saved to: Final_Data/Final_Results/pca_50.xlsx\n",
      "\n",
      "Results DataFrame:\n",
      "   period in_sample oos_sample  n_factors  explained_variance_is  \\\n",
      "0      00         a          b          5                  0.501   \n",
      "1      00         b          c          5                  0.678   \n",
      "2      00         c          d          5                  0.660   \n",
      "3      00         d          e          5                  0.580   \n",
      "4      05         a          b          4                  0.526   \n",
      "5      05         b          c          4                  0.479   \n",
      "6      05         c          d          4                  0.566   \n",
      "7      05         d          e          4                  0.685   \n",
      "8      10         a          b          4                  0.673   \n",
      "9      10         b          c          4                  0.747   \n",
      "10     10         c          d          4                  0.617   \n",
      "11     10         d          e          4                  0.575   \n",
      "12     15         a          b          5                  0.657   \n",
      "13     15         b          c          5                  0.702   \n",
      "14     15         c          d          5                  0.557   \n",
      "15     15         d          e          5                  0.657   \n",
      "\n",
      "    explained_variance_os  residual_first_pc_ev  avg_loading_correlation  \\\n",
      "0                   0.337                 0.146                    0.394   \n",
      "1                   0.442                 0.170                    0.419   \n",
      "2                   0.421                 0.088                    0.299   \n",
      "3                   0.332                 0.097                    0.274   \n",
      "4                   0.309                 0.091                    0.351   \n",
      "5                   0.383                 0.178                    0.276   \n",
      "6                   0.552                 0.189                    0.326   \n",
      "7                   0.520                 0.121                    0.633   \n",
      "8                   0.606                 0.152                    0.415   \n",
      "9                   0.427                 0.070                    0.608   \n",
      "10                  0.394                 0.118                    0.459   \n",
      "11                  0.384                 0.081                    0.451   \n",
      "12                  0.429                 0.094                    0.317   \n",
      "13                  0.288                 0.067                    0.607   \n",
      "14                  0.284                 0.319                    0.268   \n",
      "15                  0.388                 0.138                    0.502   \n",
      "\n",
      "    avg_congruence  sparsity_index  factor_1_congruence  factor_1_correlation  \\\n",
      "0            0.454           0.220                0.737                 0.475   \n",
      "1            0.464           0.207                0.853                 0.594   \n",
      "2            0.342           0.184                0.940                 0.726   \n",
      "3            0.342           0.257                0.899                 0.561   \n",
      "4            0.391           0.224                0.824                 0.590   \n",
      "5            0.359           0.216                0.809                 0.490   \n",
      "6            0.412           0.202                0.730                 0.280   \n",
      "7            0.702           0.175                0.915                 0.655   \n",
      "8            0.447           0.204                0.979                 0.854   \n",
      "9            0.632           0.220                0.913                 0.811   \n",
      "10           0.501           0.260                0.916                 0.772   \n",
      "11           0.479           0.249                0.860                 0.712   \n",
      "12           0.329           0.228                0.417                 0.077   \n",
      "13           0.625           0.246                0.818                 0.839   \n",
      "14           0.293           0.202                0.589                 0.605   \n",
      "15           0.507           0.209                0.819                 0.807   \n",
      "\n",
      "    factor_2_congruence  factor_2_correlation  factor_3_congruence  \\\n",
      "0                 0.500                 0.480                0.550   \n",
      "1                 0.654                 0.674                0.491   \n",
      "2                 0.462                 0.460                0.072   \n",
      "3                 0.087                 0.090                0.143   \n",
      "4                 0.485                 0.497                0.014   \n",
      "5                 0.356                 0.321                0.019   \n",
      "6                 0.621                 0.725                0.146   \n",
      "7                 0.685                 0.669                0.620   \n",
      "8                 0.313                 0.314                0.031   \n",
      "9                 0.709                 0.717                0.611   \n",
      "10                0.819                 0.795                0.009   \n",
      "11                0.575                 0.545                0.348   \n",
      "12                0.291                 0.546                0.502   \n",
      "13                0.632                 0.560                0.532   \n",
      "14                0.239                 0.092                0.059   \n",
      "15                0.757                 0.763                0.645   \n",
      "\n",
      "    factor_3_correlation  factor_4_congruence  factor_4_correlation  \\\n",
      "0                  0.548                0.091                 0.079   \n",
      "1                  0.506                0.003                 0.004   \n",
      "2                  0.072                0.053                 0.053   \n",
      "3                  0.141                0.344                 0.343   \n",
      "4                  0.079                0.239                 0.236   \n",
      "5                  0.040                0.253                 0.255   \n",
      "6                  0.150                0.150                 0.149   \n",
      "7                  0.622                0.588                 0.586   \n",
      "8                  0.029                0.466                 0.461   \n",
      "9                  0.613                0.297                 0.290   \n",
      "10                 0.007                0.259                 0.260   \n",
      "11                 0.416                0.133                 0.132   \n",
      "12                 0.518                0.209                 0.209   \n",
      "13                 0.499                0.588                 0.589   \n",
      "14                 0.063                0.237                 0.237   \n",
      "15                 0.643                0.125                 0.109   \n",
      "\n",
      "    factor_5_congruence  factor_5_correlation  \n",
      "0                 0.390                 0.389  \n",
      "1                 0.318                 0.314  \n",
      "2                 0.184                 0.186  \n",
      "3                 0.235                 0.236  \n",
      "4                   NaN                   NaN  \n",
      "5                   NaN                   NaN  \n",
      "6                   NaN                   NaN  \n",
      "7                   NaN                   NaN  \n",
      "8                   NaN                   NaN  \n",
      "9                   NaN                   NaN  \n",
      "10                  NaN                   NaN  \n",
      "11                  NaN                   NaN  \n",
      "12                0.227                 0.235  \n",
      "13                0.556                 0.550  \n",
      "14                0.339                 0.343  \n",
      "15                0.186                 0.189  \n",
      "\n",
      "Summary Statistics by Period:\n",
      "        IS_EV_mean  IS_EV_std  OS_EV_mean  OS_EV_std  Residual_PC1_mean  \\\n",
      "period                                                                    \n",
      "00           0.605      0.081       0.383      0.057              0.125   \n",
      "05           0.564      0.088       0.441      0.115              0.145   \n",
      "10           0.653      0.074       0.453      0.104              0.105   \n",
      "15           0.643      0.061       0.347      0.073              0.155   \n",
      "\n",
      "        Residual_PC1_std  Corr_mean  Corr_std  Congruence_mean  \\\n",
      "period                                                           \n",
      "00                 0.039      0.347     0.071            0.400   \n",
      "05                 0.047      0.396     0.161            0.466   \n",
      "10                 0.037      0.483     0.085            0.515   \n",
      "15                 0.114      0.424     0.158            0.438   \n",
      "\n",
      "        Congruence_std  Sparsity_Index_mean  Sparsity_Index_std  \n",
      "period                                                           \n",
      "00               0.068                0.217               0.031  \n",
      "05               0.159                0.204               0.022  \n",
      "10               0.081                0.233               0.026  \n",
      "15               0.156                0.221               0.020  \n",
      "\n",
      "Residuals sheet saved with columns:\n",
      "['00_a_vs_b_CCU', '00_a_vs_b_PBY', '00_a_vs_b_MWD', '00_a_vs_b_BK', '00_a_vs_b_XLNX', '00_a_vs_b_KR', '00_a_vs_b_BAX', '00_a_vs_b_MEL', '00_a_vs_b_GAS', '00_a_vs_b_DOV', '00_a_vs_b_ADM', '00_a_vs_b_LDG', '00_a_vs_b_OI', '00_a_vs_b_GT', '00_a_vs_b_WAG', '00_a_vs_b_PBI', '00_a_vs_b_ED', '00_a_vs_b_ORCL', '00_a_vs_b_PKI', '00_a_vs_b_STI', '00_a_vs_b_AFL', '00_a_vs_b_XOM', '00_a_vs_b_TIN', '00_a_vs_b_AA', '00_a_vs_b_FITB', '00_a_vs_b_DIS', '00_a_vs_b_CCL', '00_a_vs_b_AT', '00_a_vs_b_CTX', '00_a_vs_b_CCK', '00_a_vs_b_GE', '00_a_vs_b_UTX', '00_a_vs_b_PLL', '00_a_vs_b_TRB', '00_a_vs_b_COF', '00_a_vs_b_NSC', '00_a_vs_b_FDC', '00_a_vs_b_BLS', '00_a_vs_b_CBE', '00_a_vs_b_AMR', '00_a_vs_b_CB', '00_a_vs_b_FON', '00_a_vs_b_VFC', '00_a_vs_b_FLE', '00_a_vs_b_GLK', '00_a_vs_b_CPB', '00_a_vs_b_BC', '00_a_vs_b_SHW', '00_a_vs_b_DDS', '00_a_vs_b_JWN', '00_b_vs_c_CCU', '00_b_vs_c_PBY', '00_b_vs_c_MWD', '00_b_vs_c_BK', '00_b_vs_c_XLNX', '00_b_vs_c_KR', '00_b_vs_c_BAX', '00_b_vs_c_MEL', '00_b_vs_c_GAS', '00_b_vs_c_DOV', '00_b_vs_c_ADM', '00_b_vs_c_LDG', '00_b_vs_c_OI', '00_b_vs_c_GT', '00_b_vs_c_WAG', '00_b_vs_c_PBI', '00_b_vs_c_ED', '00_b_vs_c_ORCL', '00_b_vs_c_PKI', '00_b_vs_c_STI', '00_b_vs_c_AFL', '00_b_vs_c_XOM', '00_b_vs_c_TIN', '00_b_vs_c_AA', '00_b_vs_c_FITB', '00_b_vs_c_DIS', '00_b_vs_c_CCL', '00_b_vs_c_AT', '00_b_vs_c_CTX', '00_b_vs_c_CCK', '00_b_vs_c_GE', '00_b_vs_c_UTX', '00_b_vs_c_PLL', '00_b_vs_c_TRB', '00_b_vs_c_COF', '00_b_vs_c_NSC', '00_b_vs_c_FDC', '00_b_vs_c_BLS', '00_b_vs_c_CBE', '00_b_vs_c_AMR', '00_b_vs_c_CB', '00_b_vs_c_FON', '00_b_vs_c_VFC', '00_b_vs_c_FLE', '00_b_vs_c_GLK', '00_b_vs_c_CPB', '00_b_vs_c_BC', '00_b_vs_c_SHW', '00_b_vs_c_DDS', '00_b_vs_c_JWN', '00_c_vs_d_CCU', '00_c_vs_d_PBY', '00_c_vs_d_MWD', '00_c_vs_d_BK', '00_c_vs_d_XLNX', '00_c_vs_d_KR', '00_c_vs_d_BAX', '00_c_vs_d_MEL', '00_c_vs_d_GAS', '00_c_vs_d_DOV', '00_c_vs_d_ADM', '00_c_vs_d_LDG', '00_c_vs_d_OI', '00_c_vs_d_GT', '00_c_vs_d_WAG', '00_c_vs_d_PBI', '00_c_vs_d_ED', '00_c_vs_d_ORCL', '00_c_vs_d_PKI', '00_c_vs_d_STI', '00_c_vs_d_AFL', '00_c_vs_d_XOM', '00_c_vs_d_TIN', '00_c_vs_d_AA', '00_c_vs_d_FITB', '00_c_vs_d_DIS', '00_c_vs_d_CCL', '00_c_vs_d_AT', '00_c_vs_d_CTX', '00_c_vs_d_CCK', '00_c_vs_d_GE', '00_c_vs_d_UTX', '00_c_vs_d_PLL', '00_c_vs_d_TRB', '00_c_vs_d_COF', '00_c_vs_d_NSC', '00_c_vs_d_FDC', '00_c_vs_d_BLS', '00_c_vs_d_CBE', '00_c_vs_d_AMR', '00_c_vs_d_CB', '00_c_vs_d_FON', '00_c_vs_d_VFC', '00_c_vs_d_FLE', '00_c_vs_d_GLK', '00_c_vs_d_CPB', '00_c_vs_d_BC', '00_c_vs_d_SHW', '00_c_vs_d_DDS', '00_c_vs_d_JWN', '00_d_vs_e_CCU', '00_d_vs_e_PBY', '00_d_vs_e_MWD', '00_d_vs_e_BK', '00_d_vs_e_XLNX', '00_d_vs_e_KR', '00_d_vs_e_BAX', '00_d_vs_e_MEL', '00_d_vs_e_GAS', '00_d_vs_e_DOV', '00_d_vs_e_ADM', '00_d_vs_e_LDG', '00_d_vs_e_OI', '00_d_vs_e_GT', '00_d_vs_e_WAG', '00_d_vs_e_PBI', '00_d_vs_e_ED', '00_d_vs_e_ORCL', '00_d_vs_e_PKI', '00_d_vs_e_STI', '00_d_vs_e_AFL', '00_d_vs_e_XOM', '00_d_vs_e_TIN', '00_d_vs_e_AA', '00_d_vs_e_FITB', '00_d_vs_e_DIS', '00_d_vs_e_CCL', '00_d_vs_e_AT', '00_d_vs_e_CTX', '00_d_vs_e_CCK', '00_d_vs_e_GE', '00_d_vs_e_UTX', '00_d_vs_e_PLL', '00_d_vs_e_TRB', '00_d_vs_e_COF', '00_d_vs_e_NSC', '00_d_vs_e_FDC', '00_d_vs_e_BLS', '00_d_vs_e_CBE', '00_d_vs_e_AMR', '00_d_vs_e_CB', '00_d_vs_e_FON', '00_d_vs_e_VFC', '00_d_vs_e_FLE', '00_d_vs_e_GLK', '00_d_vs_e_CPB', '00_d_vs_e_BC', '00_d_vs_e_SHW', '00_d_vs_e_DDS', '00_d_vs_e_JWN', '05_a_vs_b_VLO', '05_a_vs_b_EK', '05_a_vs_b_JNY', '05_a_vs_b_HD', '05_a_vs_b_ATI', '05_a_vs_b_TER', '05_a_vs_b_MDP', '05_a_vs_b_SPG', '05_a_vs_b_JNS', '05_a_vs_b_SWK', '05_a_vs_b_CINF', '05_a_vs_b_ECL', '05_a_vs_b_LIZ', '05_a_vs_b_GILD', '05_a_vs_b_AIV', '05_a_vs_b_LLL', '05_a_vs_b_AGN', '05_a_vs_b_OXY', '05_a_vs_b_XRX', '05_a_vs_b_RTN', '05_a_vs_b_GENZ', '05_a_vs_b_PHM', '05_a_vs_b_KEY', '05_a_vs_b_THC', '05_a_vs_b_KLAC', '05_a_vs_b_CSX', '05_a_vs_b_MHP', '05_a_vs_b_INTC', '05_a_vs_b_IP', '05_a_vs_b_PCL', '05_a_vs_b_DTE', '05_a_vs_b_CTAS', '05_a_vs_b_TGT', '05_a_vs_b_EP', '05_a_vs_b_WFC', '05_a_vs_b_CNP', '05_a_vs_b_A', '05_a_vs_b_FDO', '05_a_vs_b_LNC', '05_a_vs_b_CA', '05_a_vs_b_ADP', '05_a_vs_b_HNZ', '05_a_vs_b_C', '05_a_vs_b_PBI', '05_a_vs_b_WY', '05_a_vs_b_DGX', '05_a_vs_b_MMC', '05_a_vs_b_KG', '05_a_vs_b_SNV', '05_a_vs_b_CB', '05_b_vs_c_VLO', '05_b_vs_c_EK', '05_b_vs_c_JNY', '05_b_vs_c_HD', '05_b_vs_c_ATI', '05_b_vs_c_TER', '05_b_vs_c_MDP', '05_b_vs_c_SPG', '05_b_vs_c_JNS', '05_b_vs_c_SWK', '05_b_vs_c_CINF', '05_b_vs_c_ECL', '05_b_vs_c_LIZ', '05_b_vs_c_GILD', '05_b_vs_c_AIV', '05_b_vs_c_LLL', '05_b_vs_c_AGN', '05_b_vs_c_OXY', '05_b_vs_c_XRX', '05_b_vs_c_RTN', '05_b_vs_c_GENZ', '05_b_vs_c_PHM', '05_b_vs_c_KEY', '05_b_vs_c_THC', '05_b_vs_c_KLAC', '05_b_vs_c_CSX', '05_b_vs_c_MHP', '05_b_vs_c_INTC', '05_b_vs_c_IP', '05_b_vs_c_PCL', '05_b_vs_c_DTE', '05_b_vs_c_CTAS', '05_b_vs_c_TGT', '05_b_vs_c_EP', '05_b_vs_c_WFC', '05_b_vs_c_CNP', '05_b_vs_c_A', '05_b_vs_c_FDO', '05_b_vs_c_LNC', '05_b_vs_c_CA', '05_b_vs_c_ADP', '05_b_vs_c_HNZ', '05_b_vs_c_C', '05_b_vs_c_PBI', '05_b_vs_c_WY', '05_b_vs_c_DGX', '05_b_vs_c_MMC', '05_b_vs_c_KG', '05_b_vs_c_SNV', '05_b_vs_c_CB', '05_c_vs_d_VLO', '05_c_vs_d_EK', '05_c_vs_d_JNY', '05_c_vs_d_HD', '05_c_vs_d_ATI', '05_c_vs_d_TER', '05_c_vs_d_MDP', '05_c_vs_d_SPG', '05_c_vs_d_JNS', '05_c_vs_d_SWK', '05_c_vs_d_CINF', '05_c_vs_d_ECL', '05_c_vs_d_LIZ', '05_c_vs_d_GILD', '05_c_vs_d_AIV', '05_c_vs_d_LLL', '05_c_vs_d_AGN', '05_c_vs_d_OXY', '05_c_vs_d_XRX', '05_c_vs_d_RTN', '05_c_vs_d_GENZ', '05_c_vs_d_PHM', '05_c_vs_d_KEY', '05_c_vs_d_THC', '05_c_vs_d_KLAC', '05_c_vs_d_CSX', '05_c_vs_d_MHP', '05_c_vs_d_INTC', '05_c_vs_d_IP', '05_c_vs_d_PCL', '05_c_vs_d_DTE', '05_c_vs_d_CTAS', '05_c_vs_d_TGT', '05_c_vs_d_EP', '05_c_vs_d_WFC', '05_c_vs_d_CNP', '05_c_vs_d_A', '05_c_vs_d_FDO', '05_c_vs_d_LNC', '05_c_vs_d_CA', '05_c_vs_d_ADP', '05_c_vs_d_HNZ', '05_c_vs_d_C', '05_c_vs_d_PBI', '05_c_vs_d_WY', '05_c_vs_d_DGX', '05_c_vs_d_MMC', '05_c_vs_d_KG', '05_c_vs_d_SNV', '05_c_vs_d_CB', '05_d_vs_e_VLO', '05_d_vs_e_EK', '05_d_vs_e_JNY', '05_d_vs_e_HD', '05_d_vs_e_ATI', '05_d_vs_e_TER', '05_d_vs_e_MDP', '05_d_vs_e_SPG', '05_d_vs_e_JNS', '05_d_vs_e_SWK', '05_d_vs_e_CINF', '05_d_vs_e_ECL', '05_d_vs_e_LIZ', '05_d_vs_e_GILD', '05_d_vs_e_AIV', '05_d_vs_e_LLL', '05_d_vs_e_AGN', '05_d_vs_e_OXY', '05_d_vs_e_XRX', '05_d_vs_e_RTN', '05_d_vs_e_GENZ', '05_d_vs_e_PHM', '05_d_vs_e_KEY', '05_d_vs_e_THC', '05_d_vs_e_KLAC', '05_d_vs_e_CSX', '05_d_vs_e_MHP', '05_d_vs_e_INTC', '05_d_vs_e_IP', '05_d_vs_e_PCL', '05_d_vs_e_DTE', '05_d_vs_e_CTAS', '05_d_vs_e_TGT', '05_d_vs_e_EP', '05_d_vs_e_WFC', '05_d_vs_e_CNP', '05_d_vs_e_A', '05_d_vs_e_FDO', '05_d_vs_e_LNC', '05_d_vs_e_CA', '05_d_vs_e_ADP', '05_d_vs_e_HNZ', '05_d_vs_e_C', '05_d_vs_e_PBI', '05_d_vs_e_WY', '05_d_vs_e_DGX', '05_d_vs_e_MMC', '05_d_vs_e_KG', '05_d_vs_e_SNV', '05_d_vs_e_CB', '10_a_vs_b_TEG', '10_a_vs_b_LEG', '10_a_vs_b_AZO', '10_a_vs_b_FDO', '10_a_vs_b_JEC', '10_a_vs_b_PCG', '10_a_vs_b_MS', '10_a_vs_b_ETR', '10_a_vs_b_DOV', '10_a_vs_b_DE', '10_a_vs_b_MET', '10_a_vs_b_DTV', '10_a_vs_b_UNP', '10_a_vs_b_WIN', '10_a_vs_b_ETN', '10_a_vs_b_IGT', '10_a_vs_b_SLB', '10_a_vs_b_GIS', '10_a_vs_b_SNA', '10_a_vs_b_JDSU', '10_a_vs_b_SYY', '10_a_vs_b_CHK', '10_a_vs_b_ODP', '10_a_vs_b_HCN', '10_a_vs_b_ORCL', '10_a_vs_b_TGT', '10_a_vs_b_BK', '10_a_vs_b_MSFT', '10_a_vs_b_CI', '10_a_vs_b_KR', '10_a_vs_b_COP', '10_a_vs_b_MO', '10_a_vs_b_IP', '10_a_vs_b_TER', '10_a_vs_b_AIV', '10_a_vs_b_TROW', '10_a_vs_b_AGN', '10_a_vs_b_STT', '10_a_vs_b_WEC', '10_a_vs_b_SE', '10_a_vs_b_BA', '10_a_vs_b_COH', '10_a_vs_b_AIZ', '10_a_vs_b_RRC', '10_a_vs_b_KIM', '10_a_vs_b_MMC', '10_a_vs_b_INTU', '10_a_vs_b_CSC', '10_a_vs_b_CELG', '10_a_vs_b_NOV', '10_b_vs_c_TEG', '10_b_vs_c_LEG', '10_b_vs_c_AZO', '10_b_vs_c_FDO', '10_b_vs_c_JEC', '10_b_vs_c_PCG', '10_b_vs_c_MS', '10_b_vs_c_ETR', '10_b_vs_c_DOV', '10_b_vs_c_DE', '10_b_vs_c_MET', '10_b_vs_c_DTV', '10_b_vs_c_UNP', '10_b_vs_c_WIN', '10_b_vs_c_ETN', '10_b_vs_c_IGT', '10_b_vs_c_SLB', '10_b_vs_c_GIS', '10_b_vs_c_SNA', '10_b_vs_c_JDSU', '10_b_vs_c_SYY', '10_b_vs_c_CHK', '10_b_vs_c_ODP', '10_b_vs_c_HCN', '10_b_vs_c_ORCL', '10_b_vs_c_TGT', '10_b_vs_c_BK', '10_b_vs_c_MSFT', '10_b_vs_c_CI', '10_b_vs_c_KR', '10_b_vs_c_COP', '10_b_vs_c_MO', '10_b_vs_c_IP', '10_b_vs_c_TER', '10_b_vs_c_AIV', '10_b_vs_c_TROW', '10_b_vs_c_AGN', '10_b_vs_c_STT', '10_b_vs_c_WEC', '10_b_vs_c_SE', '10_b_vs_c_BA', '10_b_vs_c_COH', '10_b_vs_c_AIZ', '10_b_vs_c_RRC', '10_b_vs_c_KIM', '10_b_vs_c_MMC', '10_b_vs_c_INTU', '10_b_vs_c_CSC', '10_b_vs_c_CELG', '10_b_vs_c_NOV', '10_c_vs_d_TEG', '10_c_vs_d_LEG', '10_c_vs_d_AZO', '10_c_vs_d_FDO', '10_c_vs_d_JEC', '10_c_vs_d_PCG', '10_c_vs_d_MS', '10_c_vs_d_ETR', '10_c_vs_d_DOV', '10_c_vs_d_DE', '10_c_vs_d_MET', '10_c_vs_d_DTV', '10_c_vs_d_UNP', '10_c_vs_d_WIN', '10_c_vs_d_ETN', '10_c_vs_d_IGT', '10_c_vs_d_SLB', '10_c_vs_d_GIS', '10_c_vs_d_SNA', '10_c_vs_d_JDSU', '10_c_vs_d_SYY', '10_c_vs_d_CHK', '10_c_vs_d_ODP', '10_c_vs_d_HCN', '10_c_vs_d_ORCL', '10_c_vs_d_TGT', '10_c_vs_d_BK', '10_c_vs_d_MSFT', '10_c_vs_d_CI', '10_c_vs_d_KR', '10_c_vs_d_COP', '10_c_vs_d_MO', '10_c_vs_d_IP', '10_c_vs_d_TER', '10_c_vs_d_AIV', '10_c_vs_d_TROW', '10_c_vs_d_AGN', '10_c_vs_d_STT', '10_c_vs_d_WEC', '10_c_vs_d_SE', '10_c_vs_d_BA', '10_c_vs_d_COH', '10_c_vs_d_AIZ', '10_c_vs_d_RRC', '10_c_vs_d_KIM', '10_c_vs_d_MMC', '10_c_vs_d_INTU', '10_c_vs_d_CSC', '10_c_vs_d_CELG', '10_c_vs_d_NOV', '10_d_vs_e_TEG', '10_d_vs_e_LEG', '10_d_vs_e_AZO', '10_d_vs_e_FDO', '10_d_vs_e_JEC', '10_d_vs_e_PCG', '10_d_vs_e_MS', '10_d_vs_e_ETR', '10_d_vs_e_DOV', '10_d_vs_e_DE', '10_d_vs_e_MET', '10_d_vs_e_DTV', '10_d_vs_e_UNP', '10_d_vs_e_WIN', '10_d_vs_e_ETN', '10_d_vs_e_IGT', '10_d_vs_e_SLB', '10_d_vs_e_GIS', '10_d_vs_e_SNA', '10_d_vs_e_JDSU', '10_d_vs_e_SYY', '10_d_vs_e_CHK', '10_d_vs_e_ODP', '10_d_vs_e_HCN', '10_d_vs_e_ORCL', '10_d_vs_e_TGT', '10_d_vs_e_BK', '10_d_vs_e_MSFT', '10_d_vs_e_CI', '10_d_vs_e_KR', '10_d_vs_e_COP', '10_d_vs_e_MO', '10_d_vs_e_IP', '10_d_vs_e_TER', '10_d_vs_e_AIV', '10_d_vs_e_TROW', '10_d_vs_e_AGN', '10_d_vs_e_STT', '10_d_vs_e_WEC', '10_d_vs_e_SE', '10_d_vs_e_BA', '10_d_vs_e_COH', '10_d_vs_e_AIZ', '10_d_vs_e_RRC', '10_d_vs_e_KIM', '10_d_vs_e_MMC', '10_d_vs_e_INTU', '10_d_vs_e_CSC', '10_d_vs_e_CELG', '10_d_vs_e_NOV', '15_a_vs_b_ETFC', '15_a_vs_b_HAS', '15_a_vs_b_HOG', '15_a_vs_b_LM', '15_a_vs_b_UHS', '15_a_vs_b_CSCO', '15_a_vs_b_ALL', '15_a_vs_b_TXN', '15_a_vs_b_DFS', '15_a_vs_b_NEE', '15_a_vs_b_PSA', '15_a_vs_b_PH', '15_a_vs_b_PVH', '15_a_vs_b_AMT', '15_a_vs_b_MNST', '15_a_vs_b_A', '15_a_vs_b_ZTS', '15_a_vs_b_MSI', '15_a_vs_b_AXP', '15_a_vs_b_OMC', '15_a_vs_b_URBN', '15_a_vs_b_MDLZ', '15_a_vs_b_MCD', '15_a_vs_b_GILD', '15_a_vs_b_EQR', '15_a_vs_b_URI', '15_a_vs_b_TMO', '15_a_vs_b_RRC', '15_a_vs_b_CELG', '15_a_vs_b_AKAM', '15_a_vs_b_WMB', '15_a_vs_b_PKI', '15_a_vs_b_GME', '15_a_vs_b_ALLE', '15_a_vs_b_AMZN', '15_a_vs_b_ED', '15_a_vs_b_KSS', '15_a_vs_b_VMC', '15_a_vs_b_DNR', '15_a_vs_b_V', '15_a_vs_b_MPC', '15_a_vs_b_TRIP', '15_a_vs_b_FOSL', '15_a_vs_b_FAST', '15_a_vs_b_VIAB', '15_a_vs_b_WDC', '15_a_vs_b_UPS', '15_a_vs_b_AIV', '15_a_vs_b_ALXN', '15_a_vs_b_NI', '15_b_vs_c_ETFC', '15_b_vs_c_HAS', '15_b_vs_c_HOG', '15_b_vs_c_LM', '15_b_vs_c_UHS', '15_b_vs_c_CSCO', '15_b_vs_c_ALL', '15_b_vs_c_TXN', '15_b_vs_c_DFS', '15_b_vs_c_NEE', '15_b_vs_c_PSA', '15_b_vs_c_PH', '15_b_vs_c_PVH', '15_b_vs_c_AMT', '15_b_vs_c_MNST', '15_b_vs_c_A', '15_b_vs_c_ZTS', '15_b_vs_c_MSI', '15_b_vs_c_AXP', '15_b_vs_c_OMC', '15_b_vs_c_URBN', '15_b_vs_c_MDLZ', '15_b_vs_c_MCD', '15_b_vs_c_GILD', '15_b_vs_c_EQR', '15_b_vs_c_URI', '15_b_vs_c_TMO', '15_b_vs_c_RRC', '15_b_vs_c_CELG', '15_b_vs_c_AKAM', '15_b_vs_c_WMB', '15_b_vs_c_PKI', '15_b_vs_c_GME', '15_b_vs_c_ALLE', '15_b_vs_c_AMZN', '15_b_vs_c_ED', '15_b_vs_c_KSS', '15_b_vs_c_VMC', '15_b_vs_c_DNR', '15_b_vs_c_V', '15_b_vs_c_MPC', '15_b_vs_c_TRIP', '15_b_vs_c_FOSL', '15_b_vs_c_FAST', '15_b_vs_c_VIAB', '15_b_vs_c_WDC', '15_b_vs_c_UPS', '15_b_vs_c_AIV', '15_b_vs_c_ALXN', '15_b_vs_c_NI', '15_c_vs_d_ETFC', '15_c_vs_d_HAS', '15_c_vs_d_HOG', '15_c_vs_d_LM', '15_c_vs_d_UHS', '15_c_vs_d_CSCO', '15_c_vs_d_ALL', '15_c_vs_d_TXN', '15_c_vs_d_DFS', '15_c_vs_d_NEE', '15_c_vs_d_PSA', '15_c_vs_d_PH', '15_c_vs_d_PVH', '15_c_vs_d_AMT', '15_c_vs_d_MNST', '15_c_vs_d_A', '15_c_vs_d_ZTS', '15_c_vs_d_MSI', '15_c_vs_d_AXP', '15_c_vs_d_OMC', '15_c_vs_d_URBN', '15_c_vs_d_MDLZ', '15_c_vs_d_MCD', '15_c_vs_d_GILD', '15_c_vs_d_EQR', '15_c_vs_d_URI', '15_c_vs_d_TMO', '15_c_vs_d_RRC', '15_c_vs_d_CELG', '15_c_vs_d_AKAM', '15_c_vs_d_WMB', '15_c_vs_d_PKI', '15_c_vs_d_GME', '15_c_vs_d_ALLE', '15_c_vs_d_AMZN', '15_c_vs_d_ED', '15_c_vs_d_KSS', '15_c_vs_d_VMC', '15_c_vs_d_DNR', '15_c_vs_d_V', '15_c_vs_d_MPC', '15_c_vs_d_TRIP', '15_c_vs_d_FOSL', '15_c_vs_d_FAST', '15_c_vs_d_VIAB', '15_c_vs_d_WDC', '15_c_vs_d_UPS', '15_c_vs_d_AIV', '15_c_vs_d_ALXN', '15_c_vs_d_NI', '15_d_vs_e_ETFC', '15_d_vs_e_HAS', '15_d_vs_e_HOG', '15_d_vs_e_LM', '15_d_vs_e_UHS', '15_d_vs_e_CSCO', '15_d_vs_e_ALL', '15_d_vs_e_TXN', '15_d_vs_e_DFS', '15_d_vs_e_NEE', '15_d_vs_e_PSA', '15_d_vs_e_PH', '15_d_vs_e_PVH', '15_d_vs_e_AMT', '15_d_vs_e_MNST', '15_d_vs_e_A', '15_d_vs_e_ZTS', '15_d_vs_e_MSI', '15_d_vs_e_AXP', '15_d_vs_e_OMC', '15_d_vs_e_URBN', '15_d_vs_e_MDLZ', '15_d_vs_e_MCD', '15_d_vs_e_GILD', '15_d_vs_e_EQR', '15_d_vs_e_URI', '15_d_vs_e_TMO', '15_d_vs_e_RRC', '15_d_vs_e_CELG', '15_d_vs_e_AKAM', '15_d_vs_e_WMB', '15_d_vs_e_PKI', '15_d_vs_e_GME', '15_d_vs_e_ALLE', '15_d_vs_e_AMZN', '15_d_vs_e_ED', '15_d_vs_e_KSS', '15_d_vs_e_VMC', '15_d_vs_e_DNR', '15_d_vs_e_V', '15_d_vs_e_MPC', '15_d_vs_e_TRIP', '15_d_vs_e_FOSL', '15_d_vs_e_FAST', '15_d_vs_e_VIAB', '15_d_vs_e_WDC', '15_d_vs_e_UPS', '15_d_vs_e_AIV', '15_d_vs_e_ALXN', '15_d_vs_e_NI']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import os, re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def congruence_coefficient(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes Tucker's coefficient of congruence between two vectors.\n",
    "    \"\"\"\n",
    "    return np.sum(v1 * v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))\n",
    "\n",
    "def compute_sparsity_index(loadings):\n",
    "    \"\"\"\n",
    "    Computes the normalized sparsity index for a given loadings DataFrame.\n",
    "    For each factor vector v, we compute the ratio:\n",
    "      ratio = ||v||_1 / ||v||_2,\n",
    "    then normalize over n (number of assets):\n",
    "      normalized = (sqrt(n) - ratio) / (sqrt(n)-1),\n",
    "    so that 1 means only one asset carries weight and 0 means all assets share equally.\n",
    "    Finally, return the average over factors.\n",
    "    \"\"\"\n",
    "    sparsity_scores = []\n",
    "    n_assets = loadings.shape[0]\n",
    "    for col in loadings.columns:\n",
    "        v = loadings[col].values\n",
    "        l1_norm = np.linalg.norm(v, 1)\n",
    "        l2_norm = np.linalg.norm(v, 2)\n",
    "        ratio = l1_norm / l2_norm if l2_norm != 0 else np.nan\n",
    "        normalized = (np.sqrt(n_assets) - ratio) / (np.sqrt(n_assets) - 1)\n",
    "        sparsity_scores.append(normalized)\n",
    "    return np.mean(sparsity_scores)\n",
    "\n",
    "def compute_turnover(loadings_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary mapping time labels to factor loading DataFrames, compute turnover \n",
    "    between adjacent periods.\n",
    "    Turnover = ||loadings_current - loadings_prev||_F / ||loadings_prev||_F.\n",
    "    Returns a list of dictionaries with keys 'transition' and 'turnover'.\n",
    "    \"\"\"\n",
    "    keys = list(loadings_dict.keys())\n",
    "    keys.sort(key=lambda x: (len(x), x))\n",
    "    turnover_results = []\n",
    "    for idx in range(1, len(keys)):\n",
    "        prev = loadings_dict[keys[idx-1]]\n",
    "        curr = loadings_dict[keys[idx]]\n",
    "        common = prev.index.intersection(curr.index)\n",
    "        if len(common) == 0:\n",
    "            continue\n",
    "        prev_aligned = prev.loc[common]\n",
    "        curr_aligned = curr.loc[common]\n",
    "        frob_diff = np.linalg.norm(curr_aligned.values - prev_aligned.values, 'fro')\n",
    "        frob_prev = np.linalg.norm(prev_aligned.values, 'fro')\n",
    "        turnover = frob_diff / frob_prev if frob_prev != 0 else np.nan\n",
    "        turnover_results.append({'transition': f\"{keys[idx-1]} -> {keys[idx]}\", 'turnover': turnover})\n",
    "    return turnover_results\n",
    "\n",
    "def oblimin_rotation(loadings_df, gamma=0.0, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Applies an oblimin (quartimin when gamma=0) rotation to the factor loadings.\n",
    "    Returns the rotated loadings DataFrame and the rotation matrix.\n",
    "    \"\"\"\n",
    "    loadings = loadings_df.to_numpy()    # (n_assets, n_factors)\n",
    "    p, m = loadings.shape\n",
    "    T = np.eye(m)\n",
    "    step = 1e-4\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        L_rot = loadings.dot(T)\n",
    "        mean_sq = np.mean(L_rot**2, axis=0, keepdims=True)\n",
    "        gradient = 4 * loadings.T.dot(L_rot * (L_rot**2 - mean_sq))\n",
    "        T_new = T - step * gradient\n",
    "        if np.linalg.norm(T_new - T) < tol:\n",
    "            T = T_new\n",
    "            break\n",
    "        T = T_new\n",
    "\n",
    "    rotated = loadings.dot(T)\n",
    "    rotated_df = pd.DataFrame(rotated, index=loadings_df.index, columns=loadings_df.columns)\n",
    "    return rotated_df, T\n",
    "\n",
    "def calc_proj_ev(df_std, loadings):\n",
    "    \"\"\"\n",
    "    Given standardized returns (df_std) and factor loadings,\n",
    "    compute predicted returns using regression-based factor score estimation,\n",
    "    and return the explained variance defined as 1 - (residual variance / total variance).\n",
    "    \"\"\"\n",
    "    inv_LL = np.linalg.inv(loadings.T.dot(loadings))\n",
    "    factor_scores = (inv_LL.dot(loadings.T.dot(df_std.T))).T\n",
    "    projected = factor_scores.dot(loadings.T)\n",
    "    residuals = df_std - projected\n",
    "    total_var = df_std.var(axis=0).mean()\n",
    "    ev = 1 - residuals.var(axis=0).mean() / total_var\n",
    "    return ev\n",
    "\n",
    "def joreskog(cov, n_factors=None, max_iter=100000, tol=1e-6, min_communal=1e-6):\n",
    "    \"\"\"\n",
    "    Robust Jöreskog's factor analysis with proper handling of communalities\n",
    "    and convergence checks.\n",
    "    \"\"\"\n",
    "    original_index = cov.index.tolist() if isinstance(cov, pd.DataFrame) else None\n",
    "    cov = cov.to_numpy() if isinstance(cov, pd.DataFrame) else cov\n",
    "\n",
    "    if not isinstance(cov, np.ndarray):\n",
    "        raise ValueError(\"Covariance matrix must be numpy array or DataFrame\")\n",
    "    if cov.shape[0] != cov.shape[1]:\n",
    "        raise ValueError(\"Covariance matrix must be square\")\n",
    "    if not np.allclose(cov, cov.T, atol=1e-9):\n",
    "        raise ValueError(\"Covariance matrix must be symmetric\")\n",
    "    \n",
    "    n_vars = cov.shape[0]\n",
    "    is_correlation = np.allclose(np.diag(cov), 1.0, atol=1e-5)\n",
    "    \n",
    "    eigvals_full, _ = np.linalg.eigh(cov)\n",
    "    tol_eigen = 1e-9 * np.max(eigvals_full)\n",
    "    if n_factors is None:\n",
    "        n_factors = int(np.sum(eigvals_full > tol_eigen))\n",
    "    else:\n",
    "        if not isinstance(n_factors, int) or n_factors <= 0:\n",
    "            raise ValueError(\"n_factors must be a positive integer\")\n",
    "        n_factors = min(n_factors, n_vars)\n",
    "    \n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(eigenvals)[::-1]\n",
    "    eigenvals = eigenvals[idx][:n_factors]\n",
    "    eigenvecs = eigenvecs[:, idx][:, :n_factors]\n",
    "    \n",
    "    beta = eigenvecs @ np.diag(np.sqrt(np.maximum(eigenvals, 0)))\n",
    "    communalities = np.sum(beta**2, axis=1)\n",
    "    if is_correlation:\n",
    "        communalities = np.clip(communalities, 0, 0.999)\n",
    "    psi = np.diag(np.maximum(np.diag(cov) - communalities, min_communal))\n",
    "    \n",
    "    iter_num = 0\n",
    "    beta_change = np.inf\n",
    "    psi_change = np.inf\n",
    "    \n",
    "    while iter_num < max_iter and (beta_change > tol or psi_change > tol):\n",
    "        sigma = beta @ beta.T + psi\n",
    "        try:\n",
    "            sigma_inv = np.linalg.inv(sigma)\n",
    "        except np.linalg.LinAlgError:\n",
    "            raise ValueError(\"Singular sigma matrix - reduce n_factors or increase min_communal\")\n",
    "        \n",
    "        middle = np.linalg.inv(np.eye(n_factors) + beta.T @ sigma_inv @ beta)\n",
    "        beta_new = cov @ sigma_inv @ beta @ middle\n",
    "        \n",
    "        communalities_new = np.sum(beta_new**2, axis=1)\n",
    "        if is_correlation:\n",
    "            communalities_new = np.clip(communalities_new, 0, 0.999)\n",
    "        psi_new_diag = np.maximum(np.diag(cov) - communalities_new, min_communal)\n",
    "        psi_new = np.diag(psi_new_diag)\n",
    "        \n",
    "        if np.any(psi_new_diag <= min_communal + 1e-6):\n",
    "            print(\"Warning: Some uniquenesses at lower bound\")\n",
    "        \n",
    "        beta_change = np.linalg.norm(beta_new - beta) / (np.linalg.norm(beta) + np.finfo(float).eps)\n",
    "        psi_change = np.linalg.norm(psi_new_diag - np.diag(psi)) / (np.linalg.norm(np.diag(psi)) + np.finfo(float).eps)\n",
    "        \n",
    "        beta = beta_new\n",
    "        psi = psi_new\n",
    "        iter_num += 1\n",
    "    \n",
    "    if iter_num == max_iter:\n",
    "        print(\"Warning: Maximum iterations reached without convergence\")\n",
    "    \n",
    "    factor_variances = np.sum(beta**2, axis=0)\n",
    "    total_variance = np.sum(np.diag(cov))\n",
    "    explained_var = factor_variances / total_variance\n",
    "    explained_cumulative = np.cumsum(explained_var)\n",
    "    \n",
    "    factor_columns = [f'PC{i+1}' for i in range(n_factors)]\n",
    "    betas_df = pd.DataFrame(beta, \n",
    "                            index=original_index if original_index is not None \n",
    "                                  else [f'var_{i}' for i in range(n_vars)],\n",
    "                            columns=factor_columns)\n",
    "    \n",
    "    return betas_df, factor_variances, explained_var, explained_cumulative\n",
    "\n",
    "# --- Revised Evaluation Function for PCA-based Model ---\n",
    "\n",
    "def evaluate_pca_os(base_corr, df_os, n_factors):\n",
    "    \"\"\"\n",
    "    Computes PCA on the in-sample correlation matrix (base_corr) and on the \n",
    "    out-of-sample returns (df_os). Also projects the OOS standardized returns\n",
    "    onto the in-sample loadings to compute residuals.\n",
    "    Returns:\n",
    "        metrics: a dictionary of evaluation metrics (explained variances, residual first PC EV,\n",
    "                 average and individual factor loading congruence, sparsity index).\n",
    "        loadings_is: the in-sample PCA loadings (used as factor weights).\n",
    "        residuals: a DataFrame of residuals computed as (standardized OOS returns - projected returns),\n",
    "                   with asset names as columns.\n",
    "    \"\"\"\n",
    "    common_assets = df_os.columns.intersection(base_corr.index)\n",
    "    if len(common_assets) == 0:\n",
    "        raise ValueError(\"No common assets between in-sample and out-of-sample data\")\n",
    "    \n",
    "    # Align data\n",
    "    base_corr_aligned = base_corr.loc[common_assets, common_assets]\n",
    "    df_os_aligned = df_os[common_assets]\n",
    "    \n",
    "    # In-sample PCA on correlation matrix\n",
    "    pca_is = PCA(n_components=n_factors)\n",
    "    pca_is.fit(base_corr_aligned)\n",
    "    loadings_is = pd.DataFrame(\n",
    "        pca_is.components_.T,\n",
    "        index=common_assets,\n",
    "        columns=[f'PC{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    explained_variance_is = pca_is.explained_variance_ratio_.sum()\n",
    "    \n",
    "    sparsity_index = compute_sparsity_index(loadings_is)\n",
    "    \n",
    "    # Out-of-sample: standardize returns and compute correlation\n",
    "    df_os_std = df_os_aligned.apply(lambda x: (x - x.mean())/x.std())\n",
    "    corr_os = df_os_std.corr()\n",
    "    pca_os = PCA(n_components=n_factors)\n",
    "    pca_os.fit(corr_os)\n",
    "    loadings_os = pd.DataFrame(\n",
    "        pca_os.components_.T,\n",
    "        index=common_assets,\n",
    "        columns=[f'PC{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    # Compute loading correlations and congruence per factor\n",
    "    correlations = []\n",
    "    congruences = []\n",
    "    for i in range(n_factors):\n",
    "        loading_is = loadings_is.iloc[:, i]\n",
    "        loading_os = loadings_os.iloc[:, i]\n",
    "        corr_val = abs(np.corrcoef(loading_is, loading_os)[0, 1])\n",
    "        cong_val = abs(congruence_coefficient(loading_is, loading_os))\n",
    "        correlations.append(corr_val)\n",
    "        congruences.append(cong_val)\n",
    "    avg_loading_correlation = np.mean(correlations)\n",
    "    avg_congruence = np.mean(congruences)\n",
    "    \n",
    "    # Projection: compute factor returns and project them back to asset space\n",
    "    factor_returns_os = df_os_std @ loadings_is\n",
    "    projected_returns_os = factor_returns_os @ loadings_is.T\n",
    "    residuals = df_os_std - projected_returns_os\n",
    "    explained_variance_os = 1 - residuals.var(axis=0).mean() / df_os_std.var(axis=0).mean()\n",
    "    \n",
    "    pca_residuals = PCA(n_components=1)\n",
    "    pca_residuals.fit(residuals)\n",
    "    residual_first_pc_ev = pca_residuals.explained_variance_ratio_[0]\n",
    "    \n",
    "    metrics = {\n",
    "        'n_factors': n_factors,\n",
    "        'explained_variance_is': explained_variance_is,\n",
    "        'explained_variance_os': explained_variance_os,\n",
    "        'residual_first_pc_ev': residual_first_pc_ev,\n",
    "        'avg_loading_correlation': avg_loading_correlation,\n",
    "        'avg_congruence': avg_congruence,\n",
    "        'sparsity_index': sparsity_index\n",
    "    }\n",
    "    for i, corr_val in enumerate(correlations):\n",
    "        metrics[f'factor_{i+1}_correlation'] = corr_val\n",
    "    for i, cong_val in enumerate(congruences):\n",
    "        metrics[f'factor_{i+1}_congruence'] = cong_val\n",
    "    \n",
    "    return metrics, loadings_is, residuals\n",
    "\n",
    "# --- Data Import Section ---\n",
    "\n",
    "directory = f'Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42'\n",
    "files = os.listdir(directory)\n",
    "pattern_corr = re.compile(r'^(df\\d{2}[a-z])_varresid_(is|os)_corr_(is|os)\\.csv$')\n",
    "pattern_resid = re.compile(r'^(df\\d{2}[a-z])_varresid_(is|os)\\.csv$')\n",
    "\n",
    "for fname in files:\n",
    "    if 'cov_' in fname:\n",
    "        continue\n",
    "    full_path = os.path.join(directory, fname)\n",
    "    \n",
    "    m = pattern_corr.match(fname)\n",
    "    if m:\n",
    "        base = m.group(1)    # e.g. \"df00a\"\n",
    "        inout = m.group(2)   # \"is\" or \"os\"\n",
    "        new_key = f\"corr{base[2:]}_{inout}\"\n",
    "        df = pd.read_csv(full_path, index_col=0)\n",
    "        globals()[new_key] = df\n",
    "        continue\n",
    "\n",
    "    m = pattern_resid.match(fname)\n",
    "    if m:\n",
    "        base = m.group(1)\n",
    "        inout = m.group(2)\n",
    "        new_key = f\"{base}_{inout}\"\n",
    "        df = pd.read_csv(full_path, index_col=0)\n",
    "        globals()[new_key] = df\n",
    "        continue\n",
    "\n",
    "# --- Main Loop for Walk-Forward Evaluation --- \n",
    "\n",
    "periods = [\"00\", \"05\", \"10\", \"15\"]\n",
    "n_factors_dict = dict(zip(periods, n_factors))  # n_factors should be defined externally\n",
    "results_list = []\n",
    "factor_weights_list = []  # storing in-sample loadings (weights) per pairing\n",
    "residuals_dict = {}       # storing residuals per pairing\n",
    "\n",
    "# Define pairings for in-sample/out-of-sample: a-b, b-c, c-d, d-e\n",
    "pairings = [(\"a\", \"b\"), (\"b\", \"c\"), (\"c\", \"d\"), (\"d\", \"e\")]\n",
    "\n",
    "print(\"Using the following number of factors per period:\")\n",
    "for period, n in n_factors_dict.items():\n",
    "    print(f\"Period {period}: {n} factors\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for period in periods:\n",
    "    for in_sample, oos_sample in pairings:\n",
    "        base_corr_key = f\"corr{period}{in_sample}_is\"   # in-sample correlation matrix\n",
    "        df_os_key = f\"df{period}{oos_sample}_os\"         # out-of-sample returns\n",
    "        if base_corr_key in globals() and df_os_key in globals():\n",
    "            try:\n",
    "                metrics, loadings_is, residuals = evaluate_pca_os(\n",
    "                    globals()[base_corr_key],\n",
    "                    globals()[df_os_key],\n",
    "                    n_factors=n_factors_dict.get(period, list(n_factors_dict.values())[0])\n",
    "                )\n",
    "                metrics['period'] = period\n",
    "                metrics['in_sample'] = in_sample\n",
    "                metrics['oos_sample'] = oos_sample\n",
    "                results_list.append(metrics)\n",
    "                # Save loadings with asset identifiers\n",
    "                loadings_is_reset = loadings_is.reset_index().rename(columns={'index': 'Asset'})\n",
    "                loadings_is_reset['period'] = period\n",
    "                loadings_is_reset['in_sample'] = in_sample\n",
    "                factor_weights_list.append(loadings_is_reset)\n",
    "                # Store residuals with a label for later aggregation\n",
    "                label = f\"{period}_{in_sample}_vs_{oos_sample}\"\n",
    "                residuals_dict[label] = residuals\n",
    "                print(f\"Completed {period}: in-sample '{in_sample}' vs out-of-sample '{oos_sample}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {period} for pairing {in_sample}-{oos_sample}: {str(e)}\")\n",
    "        else:\n",
    "            missing = [key for key in [base_corr_key, df_os_key] if key not in globals()]\n",
    "            print(f\"Skipping {period} pairing {in_sample}-{oos_sample} – missing data: {missing}\")\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    column_order = ['period', 'in_sample', 'oos_sample', 'n_factors', \n",
    "                    'explained_variance_is', 'explained_variance_os', 'residual_first_pc_ev', \n",
    "                    'avg_loading_correlation', 'avg_congruence', 'sparsity_index']\n",
    "    factor_corr_cols = sorted([col for col in results_df.columns if 'factor_' in col])\n",
    "    column_order.extend(factor_corr_cols)\n",
    "    results_df = results_df[column_order]\n",
    "    \n",
    "    numeric_columns = results_df.select_dtypes(include=[np.number]).columns\n",
    "    results_df[numeric_columns] = results_df[numeric_columns].round(3)\n",
    "    results_df = results_df.sort_values(['period', 'in_sample', 'oos_sample'])\n",
    "    \n",
    "    if factor_weights_list:\n",
    "        factor_weights_df = pd.concat(factor_weights_list, ignore_index=True)\n",
    "        factor_weight_cols = [col for col in factor_weights_df.columns if col.startswith('PC')]\n",
    "        factor_weights_df = factor_weights_df[['period', 'in_sample', 'Asset'] + factor_weight_cols]\n",
    "    \n",
    "    # Optionally, compute factor turnover here (omitted for brevity)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    excel_filename = f\"Final_Data/Final_Results/pca_{stock_sample}.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename) as writer:\n",
    "        results_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        summary_stats = results_df.groupby('period').agg({\n",
    "            'explained_variance_is': ['mean', 'std'],\n",
    "            'explained_variance_os': ['mean', 'std'],\n",
    "            'residual_first_pc_ev': ['mean', 'std'],\n",
    "            'avg_loading_correlation': ['mean', 'std'],\n",
    "            'avg_congruence': ['mean', 'std'],\n",
    "            'sparsity_index': ['mean', 'std']\n",
    "        }).round(3)\n",
    "        summary_stats.columns = ['IS_EV_mean', 'IS_EV_std',\n",
    "                                 'OS_EV_mean', 'OS_EV_std',\n",
    "                                 'Residual_PC1_mean', 'Residual_PC1_std',\n",
    "                                 'Corr_mean', 'Corr_std',\n",
    "                                 'Congruence_mean', 'Congruence_std',\n",
    "                                 'Sparsity_Index_mean', 'Sparsity_Index_std']\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary_Stats')\n",
    "        factor_summary = results_df[['period'] + factor_corr_cols].groupby('period').agg(['mean', 'std']).round(3)\n",
    "        factor_summary.to_excel(writer, sheet_name='Factor_Correlations')\n",
    "        if factor_weights_list:\n",
    "            factor_weights_df.to_excel(writer, sheet_name='Factor_Weights', index=False)\n",
    "        \n",
    "        # --- Write Residuals Sheet ---\n",
    "        if residuals_dict:\n",
    "            all_residuals = None\n",
    "            for label, resid in residuals_dict.items():\n",
    "                resid_copy = resid.copy()\n",
    "                # Rename columns to include the pairing label\n",
    "                resid_copy.columns = [f\"{label}_{col}\" for col in resid_copy.columns]\n",
    "                if all_residuals is None:\n",
    "                    all_residuals = resid_copy\n",
    "                else:\n",
    "                    all_residuals = all_residuals.join(resid_copy, how='outer')\n",
    "            all_residuals.to_excel(writer, sheet_name='Residuals', index=True)\n",
    "        \n",
    "        # --- New Aggregated Results Sheet ---\n",
    "        agg_metrics = {}\n",
    "        metric_columns = [col for col in results_df.columns if col not in ['period', 'in_sample', 'oos_sample'] and np.issubdtype(results_df[col].dtype, np.number)]\n",
    "        for col in metric_columns:\n",
    "            data = results_df[col].dropna()\n",
    "            n = len(data)\n",
    "            if n > 1:\n",
    "                mean_val = data.mean()\n",
    "                std_val = data.std()\n",
    "                sem = std_val / np.sqrt(n)\n",
    "                t_stat = stats.t.ppf(1-0.025, df=n-1)\n",
    "                ci_lower = mean_val - t_stat * sem\n",
    "                ci_upper = mean_val + t_stat * sem\n",
    "            else:\n",
    "                mean_val = data.mean()\n",
    "                std_val = np.nan\n",
    "                ci_lower = np.nan\n",
    "                ci_upper = np.nan\n",
    "            agg_metrics[col] = {\n",
    "                \"mean\": round(mean_val, 3),\n",
    "                \"std\": round(std_val, 3) if pd.notnull(std_val) else std_val,\n",
    "                \"count\": n,\n",
    "                \"CI_lower\": round(ci_lower, 3) if pd.notnull(ci_lower) else ci_lower,\n",
    "                \"CI_upper\": round(ci_upper, 3) if pd.notnull(ci_upper) else ci_upper\n",
    "            }\n",
    "        agg_df = pd.DataFrame(agg_metrics).T.reset_index().rename(columns={'index':'Metric'})\n",
    "        agg_df = agg_df[['Metric', 'mean', 'std', 'count', 'CI_lower', 'CI_upper']]\n",
    "        agg_df.to_excel(writer, sheet_name='Results_Aggregated', index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {excel_filename}\")\n",
    "    print(\"\\nResults DataFrame:\")\n",
    "    print(results_df)\n",
    "    print(\"\\nSummary Statistics by Period:\")\n",
    "    print(summary_stats)\n",
    "    if residuals_dict:\n",
    "        print(\"\\nResiduals sheet saved with columns:\")\n",
    "        print(list(all_residuals.columns))\n",
    "else:\n",
    "    print(\"No results were generated. Check your input data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
