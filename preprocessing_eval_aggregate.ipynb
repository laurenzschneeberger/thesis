{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the creation of the aggregate-level VAR cleaned correlation and return matrices used to estimate the number of factors for the evaluation. It is not the same as the generation of the walkforward test sets which is in preprocessing_eval_walkforward.ipynb.\n",
    "\n",
    "Input: cleaned no_multicoll return matrices \n",
    "Output: return and correlation matrices for (00, 05, 10, 15) timeframes and (25, 50, 75, 100) stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from functions import random_subsample2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set this parameter to (25, 50, 75, 100) and the correct filepath and then run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sample = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = pd.read_csv('Final_Data/2000_2004_clean_nomulticoll.csv')\n",
    "df00 = df00.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df05 = pd.read_csv('Final_Data/2005_2009_clean_nomulticoll.csv')\n",
    "\n",
    "# Drop duplicates based on date and ticker combination\n",
    "df05 = df05.drop_duplicates(subset=['date', 'ticker'], keep='first')\n",
    "df05.reset_index(drop=True, inplace=True)\n",
    "df05 = df05.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df10 = pd.read_csv('Final_Data/2010_2014_clean_nomulticoll.csv')\n",
    "df10 = df10.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df15 = pd.read_csv('Final_Data/2015_2019_clean_nomulticoll.csv')\n",
    "df15 = df15.pivot(index = 'date', columns = 'ticker', values = 'ret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df00, df05, df10, df15) = random_subsample2([df00, df05, df10, df15], n_samples=stock_sample, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze and impute missing data using a linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 22   1.751592   float64\n",
      "TGT                 19   1.512739   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 41\n",
      "Rows with any missing values: 41 (3.26%)\n",
      "Columns with any missing values: 2 (0.80%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 22   1.751592   float64\n",
      "TGT                 19   1.512739   float64\n",
      "CA                   0   0.000000   float64\n",
      "SLE                  0   0.000000   float64\n",
      "CMVT                 0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "RTN                  0   0.000000   float64\n",
      "LU                   0   0.000000   float64\n",
      "EMR                  0   0.000000   float64\n",
      "PEG                  0   0.000000   float64\n",
      "BSX                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "CIT                 41   3.256553   float64\n",
      "TAP                 26   2.065131   float64\n",
      "AOC                 22   1.747419   float64\n",
      "BIIB                 2   0.158856   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 91\n",
      "Rows with any missing values: 69 (5.48%)\n",
      "Columns with any missing values: 4 (1.60%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "CIT                 41   3.256553   float64\n",
      "TAP                 26   2.065131   float64\n",
      "AOC                 22   1.747419   float64\n",
      "BIIB                 2   0.158856   float64\n",
      "VLO                  0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "PPL                  0   0.000000   float64\n",
      "LUV                  0   0.000000   float64\n",
      "JWN                  0   0.000000   float64\n",
      "NE                   0   0.000000   float64\n",
      "DOV                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 20   1.589825   float64\n",
      "WAG                  1   0.079491   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 21\n",
      "Rows with any missing values: 20 (1.59%)\n",
      "Columns with any missing values: 2 (0.80%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 20   1.589825   float64\n",
      "WAG                  1   0.079491   float64\n",
      "CBS                  0   0.000000   float64\n",
      "SWK                  0   0.000000   float64\n",
      "FITB                 0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "XRAY                 0   0.000000   float64\n",
      "HD                   0   0.000000   float64\n",
      "BDX                  0   0.000000   float64\n",
      "SRCL                 0   0.000000   float64\n",
      "DIS                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "HCP                 39   3.100159   float64\n",
      "CELG                27   2.146264   float64\n",
      "VIAB                18   1.430843   float64\n",
      "CBS                 18   1.430843   float64\n",
      "STI                 16   1.271860   float64\n",
      "BBT                 16   1.271860   float64\n",
      "JEC                 15   1.192369   float64\n",
      "REGN                 1   0.079491   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 150\n",
      "Rows with any missing values: 40 (3.18%)\n",
      "Columns with any missing values: 8 (3.20%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "HCP                 39   3.100159   float64\n",
      "CELG                27   2.146264   float64\n",
      "VIAB                18   1.430843   float64\n",
      "CBS                 18   1.430843   float64\n",
      "STI                 16   1.271860   float64\n",
      "...                ...        ...       ...\n",
      "BLK                  0   0.000000   float64\n",
      "EW                   0   0.000000   float64\n",
      "JNJ                  0   0.000000   float64\n",
      "FMC                  0   0.000000   float64\n",
      "RTN                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def analyze_missing_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive missing data analysis for a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Basic missing data info\n",
    "    print(\"\\n=== Missing Data Summary ===\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Missing %': missing_pct,\n",
    "        'Data Type': df.dtypes\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(summary[summary['Missing Values'] > 0])\n",
    "    \n",
    "    # Temporal patterns\n",
    "    if 'date' in df.columns:\n",
    "        print(\"\\n=== Missing Values Over Time ===\")\n",
    "        missing_by_date = df.groupby('date').isnull().sum()\n",
    "        print(f\"Dates with most missing values:\")\n",
    "        print(missing_by_date.sum(axis=1).sort_values(ascending=False).head())\n",
    "    \n",
    "    # Missing value patterns\n",
    "    print(\"\\n=== Missing Value Patterns ===\")\n",
    "    print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Rows with any missing values: {df.isnull().any(axis=1).sum()} ({df.isnull().any(axis=1).sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Columns with any missing values: {df.isnull().any(axis=0).sum()} ({df.isnull().any(axis=0).sum()/len(df.columns)*100:.2f}%)\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(analyze_missing_data(df00))\n",
    "print(analyze_missing_data(df05))\n",
    "print(analyze_missing_data(df10))\n",
    "print(analyze_missing_data(df15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 41\n",
      "Values imputed: 41\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 91\n",
      "Values imputed: 91\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 21\n",
      "Values imputed: 21\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 150\n",
      "Values imputed: 150\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def impute_returns(df, method='linear', max_gap=100, plot_sample=True):\n",
    "    \"\"\"\n",
    "    Impute missing values in returns data using linear interpolation\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with datetime index\n",
    "    method: interpolation method ('linear', 'cubic', 'polynomial')\n",
    "    max_gap: maximum number of consecutive missing values to interpolate\n",
    "    plot_sample: whether to plot sample of interpolated values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original missing value locations\n",
    "    missing_mask = df.isnull()\n",
    "    \n",
    "    # Copy original data\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Interpolate within max_gap limit\n",
    "    df_imputed = df_imputed.interpolate(\n",
    "        method=method,\n",
    "        limit=max_gap,\n",
    "        limit_direction='both',\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    # Diagnostic information\n",
    "    total_missing = missing_mask.sum().sum()\n",
    "    still_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"\\nImputation Summary:\")\n",
    "    print(f\"Original missing values: {total_missing}\")\n",
    "    print(f\"Values imputed: {total_missing - still_missing}\")\n",
    "    print(f\"Remaining missing values: {still_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "df00 = impute_returns(df00)\n",
    "df05 = impute_returns(df05)\n",
    "df10 = impute_returns(df10)\n",
    "df15 = impute_returns(df15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Test Sets\n",
    "We begin by splitting the data into 20% chunks to get 4 test periods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of walk-forward testing, we estimate a VAR (VARMA?!) model on our training set and use the residuals as training data. Then we also use those parameters to take residuals of the test set, and use that data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Process in-sample residuals for all dataframes, including df15\n",
    "datasets = {\n",
    "    \"df00\": df00,\n",
    "    \"df05\": df05,\n",
    "    \"df10\": df10,\n",
    "    \"df15\": df15\n",
    "}\n",
    "dfs = [df00, df05, df10, df15]\n",
    "prefixes = [\"df00\", \"df05\", \"df10\", \"df15\"]\n",
    "\n",
    "# Compute and save in-sample residuals for each dataframe\n",
    "for df, prefix in zip(dfs, prefixes):\n",
    "    model = VAR(df)\n",
    "    results = model.fit(maxlags=1)\n",
    "    resid_is = results.resid.copy()\n",
    "    varname_is = f\"{prefix}_varresid_is_{stock_sample}\"\n",
    "    globals()[varname_is] = resid_is\n",
    "    filepath_is = os.path.join(\"Final_Data/Walkforward_Sets/aggregate_level\", f\"{varname_is}.csv\")\n",
    "    resid_is.to_csv(filepath_is)\n",
    "\n",
    "# Compute and save out-of-sample residuals only for pairs (i.e. for df00, df05, df10)\n",
    "for i in range(len(dfs) - 1):\n",
    "    training = dfs[i]\n",
    "    test = dfs[i + 1]\n",
    "    model = VAR(training)\n",
    "    results = model.fit(maxlags=1)\n",
    "\n",
    "    # Prepare test lag by shifting one period\n",
    "    test_lag = test.shift(1)\n",
    "    test_lag.iloc[0] = training.iloc[-1]\n",
    "\n",
    "    intercept = results.params.iloc[0]\n",
    "    lag_coef = results.params.iloc[1]\n",
    "    predicted = intercept + test_lag.multiply(lag_coef, axis=1)\n",
    "    resid_os = test - predicted\n",
    "\n",
    "    next_prefix = prefixes[i + 1]\n",
    "    varname_os = f\"{next_prefix}_varresid_os\"\n",
    "    globals()[varname_os] = resid_os\n",
    "    filepath_os = os.path.join(\"Final_Data/Walkforward_Sets/aggregate_level\", f\"{varname_os}.csv\")\n",
    "    resid_os.to_csv(filepath_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix Estimation with Ledoit-Wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import ledoit_wolf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def cov_to_corr(cov):\n",
    "    d = np.sqrt(np.diag(cov))\n",
    "    return cov / np.outer(d, d)\n",
    "\n",
    "def process_ledoit_wolf_correlations():\n",
    "    dfs = [df00, df05, df10, df15]\n",
    "    names = ['df00', 'df05', 'df10', 'df15']\n",
    "    \n",
    "    for df, name in zip(dfs, names):\n",
    "        X = df.values\n",
    "        \n",
    "        # Compute Ledoit-Wolf covariance and correlation\n",
    "        cov, shrinkage = ledoit_wolf(X)\n",
    "        corr = cov_to_corr(cov)\n",
    "        \n",
    "        # Save correlation matrix\n",
    "        corr_filename = f\"Final_Data/Walkforward_Sets/aggregate_level/{name}_varresid_is_{stock_sample}_corr_{stock_sample}.csv\"\n",
    "        pd.DataFrame(corr, index=df.columns, columns=df.columns).to_csv(corr_filename)\n",
    "\n",
    "process_ledoit_wolf_correlations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
