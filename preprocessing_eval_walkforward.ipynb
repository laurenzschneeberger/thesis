{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitive notebook describing all of the preprocessing steps we took. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sample=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrds \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.covariance import LedoitWolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pull data from WRDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Connection: schneeberger, PW: $4lauSchne!!\\n\\nconn = wrds.Connection()\\ndb = conn\\n\\ntime_periods = [\\n    (\\'2000\\', \\'2004\\', \\'2000_2004\\'),\\n    (\\'2005\\', \\'2009\\', \\'2005_2009\\'),\\n    (\\'2010\\', \\'2014\\', \\'2010_2014\\'),\\n    (\\'2015\\', \\'2019\\', \\'2015_2019\\')\\n]\\n\\nfor start_year, end_year, filename in time_periods:\\n    cap_query = f\"\"\"\\n      WITH cap_calculation AS (\\n         SELECT permno, \\n               (ABS(prc)/cfacpr) * (shrout * cfacshr) AS market_cap\\n         FROM crsp.msf\\n         WHERE date = (SELECT MAX(date) \\n                        FROM crsp.msf \\n                        WHERE date <= \\'2005-01-01\\')\\n         AND cfacpr != 0\\n         AND cfacshr != 0\\n      )\\n      SELECT permno \\n      FROM cap_calculation\\n      ORDER BY market_cap DESC \\n      LIMIT 100\\n    \"\"\"\\n    \\n    returns_query = f\"\"\"\\n        SELECT a.date, a.ret, b.ticker\\n        FROM crsp.dsf a\\n        JOIN crsp.dsenames b \\n            ON a.permno = b.permno\\n            AND a.date BETWEEN b.namedt AND b.nameendt\\n        WHERE a.permno IN ({cap_query})\\n            AND a.date BETWEEN \\'{start_year}-01-01\\' AND \\'{end_year}-12-31\\'\\n    \"\"\"\\n    \\n    returns = db.raw_sql(returns_query)\\n    returns.to_csv(f\\'Final_Data/{filename}_raw.csv\\', index=False)\\n\\ndb.close()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Connection: schneeberger, PW: $4lauSchne!!\n",
    "\n",
    "conn = wrds.Connection()\n",
    "db = conn\n",
    "\n",
    "time_periods = [\n",
    "    ('2000', '2004', '2000_2004'),\n",
    "    ('2005', '2009', '2005_2009'),\n",
    "    ('2010', '2014', '2010_2014'),\n",
    "    ('2015', '2019', '2015_2019')\n",
    "]\n",
    "\n",
    "for start_year, end_year, filename in time_periods:\n",
    "    cap_query = f\"\"\"\n",
    "      WITH cap_calculation AS (\n",
    "         SELECT permno, \n",
    "               (ABS(prc)/cfacpr) * (shrout * cfacshr) AS market_cap\n",
    "         FROM crsp.msf\n",
    "         WHERE date = (SELECT MAX(date) \n",
    "                        FROM crsp.msf \n",
    "                        WHERE date <= '2005-01-01')\n",
    "         AND cfacpr != 0\n",
    "         AND cfacshr != 0\n",
    "      )\n",
    "      SELECT permno \n",
    "      FROM cap_calculation\n",
    "      ORDER BY market_cap DESC \n",
    "      LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    returns_query = f\"\"\"\n",
    "        SELECT a.date, a.ret, b.ticker\n",
    "        FROM crsp.dsf a\n",
    "        JOIN crsp.dsenames b \n",
    "            ON a.permno = b.permno\n",
    "            AND a.date BETWEEN b.namedt AND b.nameendt\n",
    "        WHERE a.permno IN ({cap_query})\n",
    "            AND a.date BETWEEN '{start_year}-01-01' AND '{end_year}-12-31'\n",
    "    \"\"\"\n",
    "    \n",
    "    returns = db.raw_sql(returns_query)\n",
    "    returns.to_csv(f'Final_Data/{filename}_raw.csv', index=False)\n",
    "\n",
    "db.close()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = pd.read_csv('Final_Data/2000_2004_raw.csv')\n",
    "df05 = pd.read_csv('Final_Data/2005_2009_raw.csv')\n",
    "df10 = pd.read_csv('Final_Data/2010_2014_raw.csv')\n",
    "df15 = pd.read_csv('Final_Data/2015_2019_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning: remove securities with missing data, convert to log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "periods = ['2000_2004', '2005_2009', '2010_2014', '2015_2019']\n",
    "processed_dfs = {}\n",
    "\n",
    "for period in periods:\n",
    "   # Import\n",
    "   df = pd.read_csv(f'Data/{period}_raw.csv')\n",
    "   \n",
    "   # Format columns\n",
    "   df['date'] = pd.to_datetime(df['date'])\n",
    "   df['ret'] = pd.to_numeric(df['ret'], errors='coerce')\n",
    "   df['ticker'] = df['ticker'].astype(str)\n",
    "   \n",
    "   # Convert to log returns\n",
    "   df['log_ret'] = np.log(1 + df['ret'])\n",
    "   \n",
    "   # Calculate missing data percentage per ticker\n",
    "   total_days = df['date'].nunique()\n",
    "   missing_pct = df.groupby('ticker').size() / total_days\n",
    "   valid_tickers = missing_pct[missing_pct >= 0.95].index\n",
    "   \n",
    "   # Filter and store\n",
    "   df_clean = df[df['ticker'].isin(valid_tickers)]\n",
    "   processed_dfs[period] = df_clean\n",
    "   \n",
    "   # Save\n",
    "   df_clean.to_csv(f'Final_Data/{period}_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = pd.read_csv('Final_Data/2000_2004_clean.csv')\n",
    "df05 = pd.read_csv('Final_Data/2005_2009_clean.csv')\n",
    "df10 = pd.read_csv('Final_Data/2010_2014_clean.csv')\n",
    "df15 = pd.read_csv('Final_Data/2015_2019_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove highly correlated stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated(df, threshold=0.90):\n",
    "    # Convert to numeric, coerce errors to NaN\n",
    "    df_numeric = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df_numeric.corr().abs()\n",
    "    \n",
    "    # Create upper triangle mask\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find columns with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    # Drop the identified columns\n",
    "    df_cleaned = df.drop(columns=to_drop)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Load and process each dataframe\n",
    "dfs = {\n",
    "    '2000_2004': 'Final_Data/2000_2004_clean.csv',\n",
    "    '2005_2009': 'Final_Data/2005_2009_clean.csv',\n",
    "    '2010_2014': 'Final_Data/2010_2014_clean.csv',\n",
    "    '2015_2019': 'Final_Data/2015_2019_clean.csv'\n",
    "}\n",
    "\n",
    "for period, file_path in dfs.items():\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Set date as index if it exists as a column\n",
    "    if 'date' in df.columns:\n",
    "        df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Remove highly correlated securities\n",
    "    df_cleaned = remove_highly_correlated(df)\n",
    "    \n",
    "    # Save cleaned dataframe\n",
    "    output_path = f'Final_Data/{period}_clean_nomulticoll.csv'\n",
    "    df_cleaned.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and reshape. We had duplicate data issues in 2005, so we clean that out before reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = pd.read_csv('Final_Data/2000_2004_clean_nomulticoll.csv')\n",
    "df00 = df00.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df05 = pd.read_csv('Final_Data/2005_2009_clean_nomulticoll.csv')\n",
    "\n",
    "# Drop duplicates based on date and ticker combination\n",
    "df05 = df05.drop_duplicates(subset=['date', 'ticker'], keep='first')\n",
    "df05.reset_index(drop=True, inplace=True)\n",
    "df05 = df05.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df10 = pd.read_csv('Final_Data/2010_2014_clean_nomulticoll.csv')\n",
    "df10 = df10.pivot(index = 'date', columns = 'ticker', values = 'ret')\n",
    "\n",
    "df15 = pd.read_csv('Final_Data/2015_2019_clean_nomulticoll.csv')\n",
    "df15 = df15.pivot(index = 'date', columns = 'ticker', values = 'ret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import random_subsample2\n",
    "\n",
    "(df00, df05, df10, df15) = random_subsample2([df00, df05, df10, df15], n_samples=stock_sample, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze and impute missing data using a linear interpolation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 22   1.751592   float64\n",
      "TGT                 19   1.512739   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 41\n",
      "Rows with any missing values: 41 (3.26%)\n",
      "Columns with any missing values: 2 (0.80%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 22   1.751592   float64\n",
      "TGT                 19   1.512739   float64\n",
      "CA                   0   0.000000   float64\n",
      "SLE                  0   0.000000   float64\n",
      "CMVT                 0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "RTN                  0   0.000000   float64\n",
      "LU                   0   0.000000   float64\n",
      "EMR                  0   0.000000   float64\n",
      "PEG                  0   0.000000   float64\n",
      "BSX                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "CIT                 41   3.256553   float64\n",
      "TAP                 26   2.065131   float64\n",
      "AOC                 22   1.747419   float64\n",
      "BIIB                 2   0.158856   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 91\n",
      "Rows with any missing values: 69 (5.48%)\n",
      "Columns with any missing values: 4 (1.60%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "CIT                 41   3.256553   float64\n",
      "TAP                 26   2.065131   float64\n",
      "AOC                 22   1.747419   float64\n",
      "BIIB                 2   0.158856   float64\n",
      "VLO                  0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "PPL                  0   0.000000   float64\n",
      "LUV                  0   0.000000   float64\n",
      "JWN                  0   0.000000   float64\n",
      "NE                   0   0.000000   float64\n",
      "DOV                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 20   1.589825   float64\n",
      "WAG                  1   0.079491   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 21\n",
      "Rows with any missing values: 20 (1.59%)\n",
      "Columns with any missing values: 2 (0.80%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "WLP                 20   1.589825   float64\n",
      "WAG                  1   0.079491   float64\n",
      "CBS                  0   0.000000   float64\n",
      "SWK                  0   0.000000   float64\n",
      "FITB                 0   0.000000   float64\n",
      "...                ...        ...       ...\n",
      "XRAY                 0   0.000000   float64\n",
      "HD                   0   0.000000   float64\n",
      "BDX                  0   0.000000   float64\n",
      "SRCL                 0   0.000000   float64\n",
      "DIS                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n",
      "\n",
      "=== Missing Data Summary ===\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "HCP                 39   3.100159   float64\n",
      "CELG                27   2.146264   float64\n",
      "VIAB                18   1.430843   float64\n",
      "CBS                 18   1.430843   float64\n",
      "STI                 16   1.271860   float64\n",
      "BBT                 16   1.271860   float64\n",
      "JEC                 15   1.192369   float64\n",
      "REGN                 1   0.079491   float64\n",
      "\n",
      "=== Missing Value Patterns ===\n",
      "Total missing values: 150\n",
      "Rows with any missing values: 40 (3.18%)\n",
      "Columns with any missing values: 8 (3.20%)\n",
      "        Missing Values  Missing % Data Type\n",
      "ticker                                     \n",
      "HCP                 39   3.100159   float64\n",
      "CELG                27   2.146264   float64\n",
      "VIAB                18   1.430843   float64\n",
      "CBS                 18   1.430843   float64\n",
      "STI                 16   1.271860   float64\n",
      "...                ...        ...       ...\n",
      "BLK                  0   0.000000   float64\n",
      "EW                   0   0.000000   float64\n",
      "JNJ                  0   0.000000   float64\n",
      "FMC                  0   0.000000   float64\n",
      "RTN                  0   0.000000   float64\n",
      "\n",
      "[250 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def analyze_missing_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive missing data analysis for a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    # Basic missing data info\n",
    "    print(\"\\n=== Missing Data Summary ===\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        'Missing Values': missing,\n",
    "        'Missing %': missing_pct,\n",
    "        'Data Type': df.dtypes\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(summary[summary['Missing Values'] > 0])\n",
    "    \n",
    "    # Temporal patterns\n",
    "    if 'date' in df.columns:\n",
    "        print(\"\\n=== Missing Values Over Time ===\")\n",
    "        missing_by_date = df.groupby('date').isnull().sum()\n",
    "        print(f\"Dates with most missing values:\")\n",
    "        print(missing_by_date.sum(axis=1).sort_values(ascending=False).head())\n",
    "    \n",
    "    # Missing value patterns\n",
    "    print(\"\\n=== Missing Value Patterns ===\")\n",
    "    print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Rows with any missing values: {df.isnull().any(axis=1).sum()} ({df.isnull().any(axis=1).sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Columns with any missing values: {df.isnull().any(axis=0).sum()} ({df.isnull().any(axis=0).sum()/len(df.columns)*100:.2f}%)\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(analyze_missing_data(df00))\n",
    "print(analyze_missing_data(df05))\n",
    "print(analyze_missing_data(df10))\n",
    "print(analyze_missing_data(df15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 41\n",
      "Values imputed: 41\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 91\n",
      "Values imputed: 91\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 21\n",
      "Values imputed: 21\n",
      "Remaining missing values: 0\n",
      "\n",
      "Imputation Summary:\n",
      "Original missing values: 150\n",
      "Values imputed: 150\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def impute_returns(df, method='linear', max_gap=100, plot_sample=True):\n",
    "    \"\"\"\n",
    "    Impute missing values in returns data using linear interpolation\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame with datetime index\n",
    "    method: interpolation method ('linear', 'cubic', 'polynomial')\n",
    "    max_gap: maximum number of consecutive missing values to interpolate\n",
    "    plot_sample: whether to plot sample of interpolated values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store original missing value locations\n",
    "    missing_mask = df.isnull()\n",
    "    \n",
    "    # Copy original data\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Interpolate within max_gap limit\n",
    "    df_imputed = df_imputed.interpolate(\n",
    "        method=method,\n",
    "        limit=max_gap,\n",
    "        limit_direction='both',\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    # Diagnostic information\n",
    "    total_missing = missing_mask.sum().sum()\n",
    "    still_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"\\nImputation Summary:\")\n",
    "    print(f\"Original missing values: {total_missing}\")\n",
    "    print(f\"Values imputed: {total_missing - still_missing}\")\n",
    "    print(f\"Remaining missing values: {still_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "df00 = impute_returns(df00)\n",
    "df05 = impute_returns(df05)\n",
    "df10 = impute_returns(df10)\n",
    "df15 = impute_returns(df15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Test Sets\n",
    "We begin by splitting the data into 20% chunks to get 4 test periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df00, df05, df10, df15]\n",
    "suffixes = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "for i, df in enumerate(datasets):\n",
    "    base_name = f'df{str(i*5).zfill(2)}'  # Creates df00, df05, df10, df15\n",
    "    n = len(df)\n",
    "    chunk_size = n // 5\n",
    "    \n",
    "    for j, suffix in enumerate(suffixes):\n",
    "        start_idx = j * chunk_size\n",
    "        end_idx = start_idx + chunk_size if j < 4 else None  # For last chunk, include remainder\n",
    "        \n",
    "        # Create new variable name dynamically\n",
    "        new_df_name = f\"{base_name}{suffix}\"\n",
    "        globals()[new_df_name] = df.iloc[start_idx:end_idx].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of walk-forward testing, we estimate a VAR (VARMA?!) model on our training set and use the residuals as training data. Then we also use those parameters to take residuals of the test set, and use that data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "datasets = {\n",
    "    \"df00\": [df00a, df00b, df00c, df00d, df00e],\n",
    "    \"df05\": [df05a, df05b, df05c, df05d, df05e],\n",
    "    \"df10\": [df10a, df10b, df10c, df10d, df10e],\n",
    "    \"df15\": [df15a, df15b, df15c, df15d, df15e]\n",
    "}\n",
    "\n",
    "for prefix, chunk_list in datasets.items():\n",
    "    for i in range(4):\n",
    "        training = chunk_list[i]\n",
    "        test = chunk_list[i + 1]\n",
    "\n",
    "        model = VAR(training)\n",
    "        results = model.fit(maxlags=1)\n",
    "\n",
    "        resid_is = results.resid.copy()\n",
    "        varname_is = f\"{prefix}{chr(97 + i)}_varresid_is\"\n",
    "        globals()[varname_is] = resid_is\n",
    "        filepath_is = f\"Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42/{varname_is}.csv\"\n",
    "        resid_is.to_csv(filepath_is)\n",
    "\n",
    "        test_lag = test.shift(1)\n",
    "        test_lag.iloc[0] = training.iloc[-1]\n",
    "\n",
    "        intercept = results.params.iloc[0]\n",
    "        lag_coef = results.params.iloc[1]\n",
    "\n",
    "        predicted = intercept + test_lag.multiply(lag_coef, axis=1)\n",
    "        resid_os = test - predicted\n",
    "\n",
    "        varname_os = f\"{prefix}{chr(97 + i + 1)}_varresid_os\"\n",
    "        globals()[varname_os] = resid_os\n",
    "        filepath_os = f\"Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42/{varname_os}.csv\"\n",
    "        resid_os.to_csv(filepath_os)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix Estimation with Ledoit-Wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import ledoit_wolf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def cov_to_corr(cov):\n",
    "    d = np.sqrt(np.diag(cov))\n",
    "    return cov / np.outer(d, d)\n",
    "\n",
    "def process_ledoit_wolf_correlations():\n",
    "    base_prefixes = ['df00', 'df05', 'df10', 'df15']\n",
    "    letters = ['a', 'b', 'c', 'd', 'e']\n",
    "    \n",
    "    for prefix in base_prefixes:\n",
    "        for i in range(4):\n",
    "            train_name = f\"{prefix}{letters[i]}_varresid_is\"\n",
    "            test_name = f\"{prefix}{letters[i+1]}_varresid_os\"\n",
    "            train_df = globals()[train_name]\n",
    "            test_df = globals()[test_name]\n",
    "            \n",
    "            X_train = train_df.values\n",
    "            X_test = test_df.values\n",
    "            \n",
    "            cov_train, shrinkage = ledoit_wolf(X_train)\n",
    "            corr_train = cov_to_corr(cov_train)\n",
    "            corr_is_filename = f\"Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42/{train_name}_corr_is.csv\"\n",
    "            pd.DataFrame(corr_train, index=train_df.columns, columns=train_df.columns).to_csv(corr_is_filename)\n",
    "            \n",
    "            sample_cov_test = np.cov(X_test, rowvar=False)\n",
    "            avg_var = np.mean(np.diag(sample_cov_test))\n",
    "            target = avg_var * np.eye(sample_cov_test.shape[0])\n",
    "            cov_test = shrinkage * target + (1 - shrinkage) * sample_cov_test\n",
    "            corr_test = cov_to_corr(cov_test)\n",
    "            corr_os_filename = f\"Final_Data/Walkforward_Sets/{stock_sample}_stocks_seed42/{test_name}_corr_os.csv\"\n",
    "            pd.DataFrame(corr_test, index=test_df.columns, columns=test_df.columns).to_csv(corr_os_filename)\n",
    "\n",
    "process_ledoit_wolf_correlations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
